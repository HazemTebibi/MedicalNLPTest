{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf337803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ D√©compression du corpus...\n",
      " Nombre de documents trouv√©s : 53\n",
      "\n",
      "üìã √âchantillon de fichiers :\n",
      "  - Rea2001vol10iss1.txt\n",
      "  - Rea2001vol10iss2.txt\n",
      "  - Rea2001vol10iss3.txt\n",
      "  - Rea2001vol10iss4.txt\n",
      "  - Rea2001vol10iss5.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1.1 D√©compression et inventaire des fichiers\n",
    "corpus_path = \"C:\\\\Users\\\\setup\\\\Desktop\\\\Reanimation.zip\"\n",
    "extract_path = \"C:\\\\Users\\\\setup\\\\Desktop\\\\medical_corpus\"\n",
    "\n",
    "print(\"üìÇ D√©compression du corpus...\")\n",
    "import zipfile\n",
    "with zipfile.ZipFile(corpus_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# 1.2 Inventaire des fichiers\n",
    "txt_files = glob.glob(f\"{extract_path}/**/*.txt\", recursive=True)\n",
    "print(f\" Nombre de documents trouv√©s : {len(txt_files)}\")\n",
    "\n",
    "# Affichage de quelques fichiers\n",
    "print(\"\\nüìã √âchantillon de fichiers :\")\n",
    "for file_path in txt_files[:5]:\n",
    "    print(f\"  - {os.path.basename(file_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Chargement des documents...\n",
      "‚úÖ Corpus charg√© : 53 documents\n",
      "\n",
      "üîç Aper√ßu du premier document :\n",
      "ID: doc_0000\n",
      "Longueur: 351869 caract√®res\n",
      "Extrait: DITORIAL \n",
      "Ranimation  Urgences devient Ranimation \n",
      "F. Schneider, 1 and J. F. Dhainaut2\n",
      "1 Service de ranimation mdicale, hpital de Hautepierre, avenue Molire, 67098, Strasbourg, France 2 groupe hospitalier cochin, hpital Saint-Jacques, 27 rue du Faubourg-Saint-Jacques, 75674, Paris cedex 14, France \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Chargement des documents dans la structure Python\n",
    "print(\" Chargement des documents...\")\n",
    "corpus = {}\n",
    "\n",
    "for i, file_path in enumerate(txt_files):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "            doc_id = f\"doc_{i:04d}\"\n",
    "            corpus[doc_id] = content\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur avec {file_path}: {e}\")\n",
    "\n",
    "print(f\" Corpus charg√© : {len(corpus)} documents\")\n",
    "\n",
    "# 1.4 V√©rification du chargement\n",
    "print(\"\\n Aper√ßu du premier document :\")\n",
    "first_doc_id = list(corpus.keys())[0]\n",
    "print(f\"ID: {first_doc_id}\")\n",
    "print(f\"Longueur: {len(corpus[first_doc_id])} caract√®res\")\n",
    "print(f\"Extrait: {corpus[first_doc_id][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79860f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Nettoyage des √©l√©ments non-textuels...\n",
      "‚úÖ Nettoyage termin√©\n",
      "\n",
      "üìä Statistiques corpus :\n",
      "‚Ä¢ Documents charg√©s : 53\n",
      "‚Ä¢ Longueur moyenne : 136972 caract√®res\n",
      "‚Ä¢ Longueur min/max : 9914 / 358630 caract√®res\n"
     ]
    }
   ],
   "source": [
    "# 1.5 Nettoyage basique (suppression √©l√©ments non-textuels)\n",
    "print(\"üßπ Nettoyage des √©l√©ments non-textuels...\")\n",
    "\n",
    "def clean_non_textual_elements(text):\n",
    "    # Supprimer les balises HTML simples\n",
    "    import re\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Supprimer les URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Supprimer les emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# Application du nettoyage\n",
    "for doc_id in corpus:\n",
    "    corpus[doc_id] = clean_non_textual_elements(corpus[doc_id])\n",
    "\n",
    "print(\" Nettoyage termin√©\")\n",
    "\n",
    "# V√©rification apr√®s nettoyage\n",
    "print(f\"\\n Statistiques corpus :\")\n",
    "doc_lengths = [len(content) for content in corpus.values()]\n",
    "print(f\"‚Ä¢ Documents charg√©s : {len(corpus)}\")\n",
    "print(f\"‚Ä¢ Longueur moyenne : {sum(doc_lengths)/len(doc_lengths):.0f} caract√®res\")\n",
    "print(f\"‚Ä¢ Longueur min/max : {min(doc_lengths)} / {max(doc_lengths)} caract√®res\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69d658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Application de la normalisation...\n",
      "‚úÖ Normalisation termin√©e\n",
      "\n",
      " Avant/Apr√®s normalisation :\n",
      "AVANT: DITORIAL \n",
      "Ranimation  Urgences devient Ranimation \n",
      "F. Schneider, 1 and J. F. Dhainaut2\n",
      "1 Service de ranimation mdicale, hpital de Hautepierre, avenue Molire, 67098, Strasbourg, France 2 groupe hospita\n",
      "APR√àS: ditorial ranimation urgences devient ranimation f schneider and j f dhai MEDICAL ut service de ranimation mdicale hpital de hautepierre avenue molire strasbourg france groupe hospitalier cochin hpital\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalisation des caract√®res accentu√©s (√© ‚Üí e, √ß ‚Üí c, etc.)\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "    \n",
    "\n",
    "    # On garde d'abord les termes m√©dicaux sp√©ciaux\n",
    "    medical_pattern = r'(\\b(?:o2|fio2|ph|peep|pam|fc|fr|sao2|pas|pad|ecg|eeg|avc|irc|ira|spo2|pao2|paco2|bic|na|k|cl|crp|vs|tp|inr|vv|vm|pc|pceep|vt|ve|ie|frv|pmax|pplat|auto-peep)\\b)'\n",
    "    \n",
    "\n",
    "    medical_terms = re.findall(medical_pattern, text)\n",
    "    placeholder_dict = {}\n",
    "    for i, term in enumerate(set(medical_terms)):\n",
    "        placeholder = f'__MEDICAL_{i}__'\n",
    "        placeholder_dict[placeholder] = term\n",
    "        text = text.replace(term, placeholder)\n",
    "    \n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Restauration des termes m√©dicaux\n",
    "    for placeholder, term in placeholder_dict.items():\n",
    "        text = text.replace(placeholder, term)\n",
    "    \n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"üîÑ Application de la normalisation...\")\n",
    "corpus_cleaned = {}\n",
    "for doc_id, content in corpus.items():\n",
    "    corpus_cleaned[doc_id] = normalize_text(content)\n",
    "\n",
    "print(\"‚úÖ Normalisation termin√©e\")\n",
    "\n",
    "# V√©rification\n",
    "print(\"\\n Avant/Apr√®s normalisation :\")\n",
    "sample_doc = list(corpus.keys())[0]\n",
    "print(\"AVANT:\", corpus[sample_doc][:200])\n",
    "print(\"APR√àS:\", corpus_cleaned[sample_doc][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3423784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation et tokenisation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\setup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\setup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokenisation termin√©e\n",
      "\n",
      " Document doc_0000:\n",
      "‚Ä¢ Phrases: 1\n",
      "‚Ä¢ Tokens: 31560\n",
      "‚Ä¢ Exemple tokens: ['ditorial', 'ranimation', 'urgences', 'devient', 'ranimation', 'f', 'schneider', 'and', 'j', 'f']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"Segmentation et tokenisation...\")\n",
    "\n",
    "corpus_tokenized = {}\n",
    "\n",
    "for doc_id, text in corpus_cleaned.items():\n",
    "    # Segmentation en phrases\n",
    "    sentences = sent_tokenize(text, language='french')\n",
    "    \n",
    "    # Tokenisation en mots pour chaque phrase\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence, language='french')\n",
    "        tokens.extend(words)\n",
    "    \n",
    "    corpus_tokenized[doc_id] = {\n",
    "        'sentences': sentences,\n",
    "        'tokens': tokens,\n",
    "        'original_text': text\n",
    "    }\n",
    "\n",
    "print(\" Tokenisation termin√©e\")\n",
    "\n",
    "# V√©rification\n",
    "sample_doc_id = list(corpus_tokenized.keys())[0]\n",
    "print(f\"\\n Document {sample_doc_id}:\")\n",
    "print(f\"‚Ä¢ Phrases: {len(corpus_tokenized[sample_doc_id]['sentences'])}\")\n",
    "print(f\"‚Ä¢ Tokens: {len(corpus_tokenized[sample_doc_id]['tokens'])}\")\n",
    "print(f\"‚Ä¢ Exemple tokens: {corpus_tokenized[sample_doc_id]['tokens'][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e011803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrage des stopwords...\n",
      "Filtrage termin√©\n",
      "\n",
      " Avant/Apr√®s filtrage - Document doc_0000:\n",
      "‚Ä¢ Tokens avant: 31560\n",
      "‚Ä¢ Tokens apr√®s: 20113\n",
      "‚Ä¢ R√©duction: 11447 tokens supprim√©s\n",
      "‚Ä¢ Exemple tokens filtr√©s: ['ditorial', 'ranimation', 'urgences', 'devient', 'ranimation', 'schneider', 'and', 'dhai', 'MEDICAL', 'ut', 'service', 'ranimation', 'mdicale', 'hpital', 'hautepierre']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\setup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Liste des stopwords fran√ßais\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "\n",
    "# Liste blanche m√©dicale (termes √† conserver m√™me s'ils sont courts)\n",
    "MEDICAL_WHITELIST = {\n",
    "    'o2', 'fio2', 'ph', 'peep', 'pam', 'fc', 'fr', 'sao2', 'pas', 'pad',\n",
    "    'ecg', 'eeg', 'avc', 'irc', 'ira', 'spo2', 'pao2', 'paco2', 'bic',\n",
    "    'na', 'k', 'cl', 'crp', 'vs', 'tp', 'inr', 'vv', 'vm', 'pc', 'pceep',\n",
    "    'vt', 've', 'ie', 'frv', 'pmax', 'pplat', 'auto-peep'\n",
    "}\n",
    "\n",
    "# Retirer les termes m√©dicaux de la liste des stopwords\n",
    "french_stopwords = french_stopwords - MEDICAL_WHITELIST\n",
    "\n",
    "print(\"Filtrage des stopwords...\")\n",
    "\n",
    "corpus_filtered = {}\n",
    "\n",
    "for doc_id, doc_data in corpus_tokenized.items():\n",
    "    filtered_tokens = [\n",
    "        token for token in doc_data['tokens'] \n",
    "        if token not in french_stopwords and len(token) > 1\n",
    "    ]\n",
    "    \n",
    "    corpus_filtered[doc_id] = {\n",
    "        'original_tokens': doc_data['tokens'],\n",
    "        'filtered_tokens': filtered_tokens,\n",
    "        'sentences': doc_data['sentences']\n",
    "    }\n",
    "\n",
    "print(\"Filtrage termin√©\")\n",
    "\n",
    "\n",
    "sample_doc_id = list(corpus_filtered.keys())[0]\n",
    "print(f\"\\n Avant/Apr√®s filtrage - Document {sample_doc_id}:\")\n",
    "print(f\"‚Ä¢ Tokens avant: {len(corpus_tokenized[sample_doc_id]['tokens'])}\")\n",
    "print(f\"‚Ä¢ Tokens apr√®s: {len(corpus_filtered[sample_doc_id]['filtered_tokens'])}\")\n",
    "print(f\"‚Ä¢ R√©duction: {len(corpus_tokenized[sample_doc_id]['tokens']) - len(corpus_filtered[sample_doc_id]['filtered_tokens'])} tokens supprim√©s\")\n",
    "print(f\"‚Ä¢ Exemple tokens filtr√©s: {corpus_filtered[sample_doc_id]['filtered_tokens'][:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159495f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chargement du mod√®le spaCy fran√ßais...\n",
      " Lemmatisation en cours...\n",
      "Lot 1/2 trait√©\n",
      "Lot 2/2 trait√©\n",
      " Lemmatisation termin√©e !\n",
      "\n",
      " Avant/Apr√®s lemmatisation - Document doc_0000:\n",
      "‚Ä¢ Tokens originaux: ['ditorial', 'ranimation', 'urgences', 'devient', 'ranimation', 'schneider', 'and', 'dhai', 'MEDICAL', 'ut']\n",
      "‚Ä¢ Lemmes: ['ditorial', 'ranimation', 'urgence', 'devenir', 'ranimation', 'schneider', 'and', 'dher', 'medical', 'ut']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "print(\" Chargement du mod√®le spaCy fran√ßais...\")\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "print(\" Lemmatisation en cours...\")\n",
    "\n",
    "corpus_lemmatized = {}\n",
    "\n",
    "batch_size = 50\n",
    "doc_ids = list(corpus_filtered.keys())\n",
    "\n",
    "for i in range(0, len(doc_ids), batch_size):\n",
    "    batch_ids = doc_ids[i:i + batch_size]\n",
    "    batch_texts = [' '.join(corpus_filtered[doc_id]['filtered_tokens']) for doc_id in batch_ids]\n",
    "    \n",
    "    docs = nlp.pipe(batch_texts, disable=[\"parser\", \"ner\"])\n",
    "    \n",
    "    for j, doc in enumerate(docs):\n",
    "        doc_id = batch_ids[j]\n",
    "        lemmas = [token.lemma_ for token in doc if token.lemma_.strip()]\n",
    "        \n",
    "        corpus_lemmatized[doc_id] = {\n",
    "            'lemmas': lemmas,\n",
    "            'original_tokens': corpus_filtered[doc_id]['filtered_tokens'],\n",
    "            'sentences': corpus_filtered[doc_id]['sentences']\n",
    "        }\n",
    "    \n",
    "    print(f\"Lot {i//batch_size + 1}/{(len(doc_ids)-1)//batch_size + 1} trait√©\")\n",
    "\n",
    "print(\" Lemmatisation termin√©e !\")\n",
    "\n",
    "\n",
    "sample_doc_id = list(corpus_lemmatized.keys())[0]\n",
    "print(f\"\\n Avant/Apr√®s lemmatisation - Document {sample_doc_id}:\")\n",
    "print(f\"‚Ä¢ Tokens originaux: {corpus_lemmatized[sample_doc_id]['original_tokens'][:10]}\")\n",
    "print(f\"‚Ä¢ Lemmes: {corpus_lemmatized[sample_doc_id]['lemmas'][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289835c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construction du vocabulaire...\n",
      "‚úÖ Vocabulaire construit: 32694 termes uniques\n",
      " Statistiques du vocabulaire:\n",
      "‚Ä¢ Termes uniques: 32694\n",
      "‚Ä¢ Documents: 53\n",
      "\n",
      "üîù 10 termes les plus fr√©quents:\n",
      "  celui: appara√Æt dans 53 documents (IDF: 0.981)\n",
      "  seul: appara√Æt dans 53 documents (IDF: 0.981)\n",
      "  auteur: appara√Æt dans 53 documents (IDF: 0.981)\n",
      "  partir: appara√Æt dans 53 documents (IDF: 0.981)\n",
      "  grand: appara√Æt dans 53 documents (IDF: 0.981)\n",
      "  tre: appara√Æt dans 53 documents (IDF: 0.981)\n",
      "  sans: appara√Æt dans 53 documents (IDF: 0.981)\n",
      "  mme: appara√Æt dans 53 documents (IDF: 0.981)\n",
      "  ranimation: appara√Æt dans 53 documents (IDF: 0.981)\n",
      "  donc: appara√Æt dans 53 documents (IDF: 0.981)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "print(\"Construction du vocabulaire...\")\n",
    "\n",
    "\n",
    "term_frequency = defaultdict(dict)  \n",
    "document_frequency = defaultdict(int) \n",
    "vocabulary = set()\n",
    "\n",
    "for doc_id, doc_data in corpus_lemmatized.items():\n",
    "    lemmas = doc_data['lemmas']\n",
    "    \n",
    "\n",
    "    doc_term_count = defaultdict(int)\n",
    "    for lemma in lemmas:\n",
    "        doc_term_count[lemma] += 1\n",
    "        vocabulary.add(lemma)\n",
    "    \n",
    "\n",
    "    for term, count in doc_term_count.items():\n",
    "        term_frequency[term][doc_id] = count\n",
    "        document_frequency[term] += 1\n",
    "\n",
    "print(f\" Vocabulaire construit: {len(vocabulary)} termes uniques\")\n",
    "\n",
    "# Calcul IDF\n",
    "N = len(corpus_lemmatized) \n",
    "idf = {}\n",
    "\n",
    "for term in vocabulary:\n",
    "    df = document_frequency[term]\n",
    "    idf[term] = math.log(N / (df + 1)) + 1  #formule mte3 idf\n",
    "\n",
    "print(f\" Statistiques du vocabulaire:\")\n",
    "print(f\"‚Ä¢ Termes uniques: {len(vocabulary)}\")\n",
    "print(f\"‚Ä¢ Documents: {N}\")\n",
    "\n",
    "\n",
    "top_terms = sorted(vocabulary, key=lambda x: document_frequency[x], reverse=True)[:10]\n",
    "print(f\"\\n 10 termes les plus fr√©quents:\")\n",
    "for term in top_terms:\n",
    "    print(f\"  {term}: appara√Æt dans {document_frequency[term]} documents (IDF: {idf[term]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46151aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Construction de l'index invers√©...\n",
      " Index invers√© construit\n",
      "Sauvegarde des fichiers...\n",
      " Fichiers sauvegard√©s:\n",
      "   - inverted_index_sample.json (√©chantillon)\n",
      "   - inverted_index.pkl (index complet)\n",
      "   - vocab.csv (vocabulaire et pond√©rations)\n"
     ]
    }
   ],
   "source": [
    "print(\" Construction de l'index invers√©...\")\n",
    "\n",
    "inverted_index = {}\n",
    "\n",
    "for doc_id, doc_data in corpus_lemmatized.items():\n",
    "    lemmas = doc_data['lemmas']\n",
    "    \n",
    "    for position, lemma in enumerate(lemmas):\n",
    "        if lemma not in inverted_index:\n",
    "            inverted_index[lemma] = {}\n",
    "        \n",
    "        if doc_id not in inverted_index[lemma]:\n",
    "            inverted_index[lemma][doc_id] = {\n",
    "                'positions': [],\n",
    "                'tf': 0\n",
    "            }\n",
    "        \n",
    "        inverted_index[lemma][doc_id]['positions'].append(position)\n",
    "        inverted_index[lemma][doc_id]['tf'] = len(inverted_index[lemma][doc_id]['positions'])\n",
    "\n",
    "print(\" Index invers√© construit\")\n",
    "\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "print(\"Sauvegarde des fichiers...\")\n",
    "\n",
    "\n",
    "index_sample = {k: v for k, v in list(inverted_index.items())[:10]}\n",
    "with open('inverted_index_sample.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(index_sample, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "with open('inverted_index.pkl', 'wb') as f:\n",
    "    pickle.dump(inverted_index, f)\n",
    "\n",
    "\n",
    "vocab_data = []\n",
    "for term in vocabulary:\n",
    "    vocab_data.append({\n",
    "        'term': term,\n",
    "        'document_frequency': document_frequency[term],\n",
    "        'idf': idf[term]\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "df_vocab = pd.DataFrame(vocab_data)\n",
    "df_vocab.to_csv('vocab.csv', index=False)\n",
    "\n",
    "print(\" Fichiers sauvegard√©s:\")\n",
    "print(\"   - inverted_index_sample.json (√©chantillon)\")\n",
    "print(\"   - inverted_index.pkl (index complet)\")\n",
    "print(\"   - vocab.csv (vocabulaire et pond√©rations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cacaf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initialisation de BM25...\n",
      "\n",
      " Test de recherche: 'ventilation m√©canique pression'\n",
      "‚Ä¢ Termes recherch√©s: ['ventilation', 'mecaniqu', 'pression']\n",
      "‚Ä¢ Top 10 r√©sultats:\n",
      "  1. doc_0046 (score: 0.2920)\n",
      "  2. doc_0039 (score: 0.2918)\n",
      "  3. doc_0035 (score: 0.2917)\n",
      "  4. doc_0007 (score: 0.2910)\n",
      "  5. doc_0027 (score: 0.2903)\n",
      "  6. doc_0024 (score: 0.2900)\n",
      "  7. doc_0000 (score: 0.2900)\n",
      "  8. doc_0008 (score: 0.2897)\n",
      "  9. doc_0001 (score: 0.2888)\n",
      "  10. doc_0016 (score: 0.2848)\n"
     ]
    }
   ],
   "source": [
    "class BM25:\n",
    "    def __init__(self, inverted_index, corpus_lemmatized, k1=1.2, b=0.75):\n",
    "        self.inverted_index = inverted_index\n",
    "        self.corpus_lemmatized = corpus_lemmatized\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.N = len(corpus_lemmatized)\n",
    "        \n",
    "\n",
    "        total_length = sum(len(doc_data['lemmas']) for doc_data in corpus_lemmatized.values())\n",
    "        self.avg_dl = total_length / self.N\n",
    "        \n",
    "\n",
    "        self.idf = {}\n",
    "        for term in inverted_index:\n",
    "            df = len(inverted_index[term])\n",
    "            self.idf[term] = math.log((self.N - df + 0.5) / (df + 0.5) + 1)\n",
    "    \n",
    "    def score(self, query_terms, doc_id):\n",
    "        score = 0.0\n",
    "        doc_length = len(self.corpus_lemmatized[doc_id]['lemmas'])\n",
    "        \n",
    "        for term in query_terms:\n",
    "            if term in self.inverted_index and doc_id in self.inverted_index[term]:\n",
    "                tf = self.inverted_index[term][doc_id]['tf']\n",
    "                df = len(self.inverted_index[term])\n",
    "                \n",
    "                # Formule BM25\n",
    "                idf = self.idf[term]\n",
    "                numerator = tf * (self.k1 + 1)\n",
    "                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avg_dl))\n",
    "                \n",
    "                score += idf * (numerator / denominator)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def search(self, query, top_k=10):\n",
    "\n",
    "        query_cleaned = normalize_text(query)\n",
    "        query_tokens = word_tokenize(query_cleaned, language='french')\n",
    "        query_filtered = [token for token in query_tokens if token not in french_stopwords and len(token) > 1]\n",
    "\n",
    "        query_doc = nlp(' '.join(query_filtered))\n",
    "        query_lemmas = [token.lemma_ for token in query_doc if token.lemma_.strip()]\n",
    "        \n",
    "\n",
    "        scores = []\n",
    "        for doc_id in self.corpus_lemmatized:\n",
    "            doc_score = self.score(query_lemmas, doc_id)\n",
    "            if doc_score > 0:\n",
    "                scores.append((doc_id, doc_score))\n",
    "        \n",
    "\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return scores[:top_k], query_lemmas\n",
    "\n",
    "print(\" Initialisation de BM25...\")\n",
    "bm25 = BM25(inverted_index, corpus_lemmatized)\n",
    "\n",
    "\n",
    "test_query = \"ventilation m√©canique pression\"\n",
    "results, processed_terms = bm25.search(test_query)\n",
    "\n",
    "print(f\"\\n Test de recherche: '{test_query}'\")\n",
    "print(f\"‚Ä¢ Termes recherch√©s: {processed_terms}\")\n",
    "print(f\"‚Ä¢ Top {len(results)} r√©sultats:\")\n",
    "for i, (doc_id, score) in enumerate(results, 1):\n",
    "    print(f\"  {i}. {doc_id} (score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e2e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 22:28:58.527 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.528 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.528 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.532 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.533 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.533 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.533 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.534 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.534 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.534 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.537 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.537 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.538 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.539 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.541 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.541 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.545 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.546 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.546 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.546 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.547 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.547 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.548 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.548 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.548 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.549 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.549 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-21 22:28:58.549 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=1, _parent=DeltaGenerator())"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Moteur de Recherche M√©dicale\",\n",
    "    page_icon=\"üîç\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "\n",
    "st.title(\" Moteur de Recherche M√©dicale \")\n",
    "st.markdown(\"**Syst√®me d'indexation**\")\n",
    "\n",
    "\n",
    "if 'data_loaded' not in st.session_state:\n",
    "    st.session_state.data_loaded = False\n",
    "if 'inverted_index' not in st.session_state:\n",
    "    st.session_state.inverted_index = None\n",
    "if 'search_engine' not in st.session_state:\n",
    "    st.session_state.search_engine = None\n",
    "\n",
    "class MedicalSearchEngine:\n",
    "    def __init__(self, inverted_index):\n",
    "        self.inverted_index = inverted_index\n",
    "        self.doc_ids = self._get_all_doc_ids()\n",
    "        self.N = len(self.doc_ids)\n",
    "        self.avg_dl = self._calculate_avg_document_length()\n",
    "        \n",
    "    def _get_all_doc_ids(self):\n",
    "        \"\"\"Extrait tous les IDs de documents\"\"\"\n",
    "        doc_ids = set()\n",
    "        for term, docs in self.inverted_index.items():\n",
    "            doc_ids.update(docs.keys())\n",
    "        return list(doc_ids)\n",
    "    \n",
    "    def _calculate_avg_document_length(self):\n",
    "        \"\"\"Calcule la longueur moyenne des documents\"\"\"\n",
    "        total_length = 0\n",
    "        for doc_id in self.doc_ids:\n",
    "            doc_length = 0\n",
    "            for term_data in self.inverted_index.values():\n",
    "                if doc_id in term_data:\n",
    "                    doc_length += term_data[doc_id]['tf']\n",
    "            total_length += doc_length\n",
    "        return total_length / self.N if self.N > 0 else 0\n",
    "    \n",
    "    def preprocess_query(self, query):\n",
    "        \"\"\"Pr√©traite la requ√™te utilisateur\"\"\"\n",
    "\n",
    "        query = query.lower()\n",
    "        query = unicodedata.normalize('NFKD', query)\n",
    "        query = ''.join([c for c in query if not unicodedata.combining(c)])\n",
    "        query = re.sub(r'[^a-zA-Z\\s]', ' ', query)\n",
    "        query = re.sub(r'\\s+', ' ', query).strip()\n",
    "        \n",
    "\n",
    "        tokens = word_tokenize(query, language='french')\n",
    "        \n",
    " \n",
    "        french_stopwords = set(stopwords.words('french'))\n",
    "        medical_whitelist = {'o2', 'fio2', 'ph', 'peep', 'pam', 'fc', 'fr', 'sao2'}\n",
    "        french_stopwords = french_stopwords - medical_whitelist\n",
    "        \n",
    "        filtered_tokens = [\n",
    "            token for token in tokens \n",
    "            if token not in french_stopwords and len(token) > 1\n",
    "        ]\n",
    "        return filtered_tokens\n",
    "    \n",
    "    def search(self, query, top_k=10, k1=1.2, b=0.75):\n",
    "        \"\"\"Recherche avec BM25\"\"\"\n",
    "        query_terms = self.preprocess_query(query)\n",
    "        \n",
    "        if not query_terms:\n",
    "            return [], query_terms\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for doc_id in self.doc_ids:\n",
    "            score = 0\n",
    "            doc_length = 0\n",
    "            \n",
    "\n",
    "            for term_data in self.inverted_index.values():\n",
    "                if doc_id in term_data:\n",
    "                    doc_length += term_data[doc_id]['tf']\n",
    "            \n",
    "            for term in query_terms:\n",
    "                if term in self.inverted_index and doc_id in self.inverted_index[term]:\n",
    "                    tf = self.inverted_index[term][doc_id]['tf']\n",
    "                    df = len(self.inverted_index[term])\n",
    "                    idf = max(0, math.log((self.N - df + 0.5) / (df + 0.5) + 1))\n",
    "                    \n",
    "                    # Formule BM25\n",
    "                    numerator = tf * (k1 + 1)\n",
    "                    denominator = tf + k1 * (1 - b + b * (doc_length / self.avg_dl))\n",
    "                    \n",
    "                    if denominator > 0:\n",
    "                        score += idf * (numerator / denominator)\n",
    "            \n",
    "            if score > 0:\n",
    "                scores.append((doc_id, score, doc_length))\n",
    "        \n",
    "        # Tri par score d√©croissant\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:top_k], query_terms\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Charge les donn√©es de recherche\"\"\"\n",
    "    try:\n",
    "        with open(\"inverted_index.pkl\", \"rb\") as f:\n",
    "            inverted_index = pickle.load(f)\n",
    "        \n",
    "        vocab_df = pd.read_csv(\"vocab.csv\") if os.path.exists(\"vocab.csv\") else None\n",
    "        docs_df = pd.read_csv(\"docs.csv\") if os.path.exists(\"docs.csv\") else None\n",
    "        \n",
    "        search_engine = MedicalSearchEngine(inverted_index)\n",
    "        return search_engine, vocab_df, docs_df, inverted_index\n",
    "    \n",
    "    except Exception as e:\n",
    "        st.error(f\" Erreur lors du chargement: {str(e)}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Sidebar pour le chargement\n",
    "with st.sidebar:\n",
    "    st.header(\" Configuration\")\n",
    "    \n",
    "    if st.button(\" Charger les donn√©es de recherche\", use_container_width=True):\n",
    "        with st.spinner(\"Chargement en cours...\"):\n",
    "            search_engine, vocab_df, docs_df, inverted_index = load_data()\n",
    "            \n",
    "            if search_engine is not None:\n",
    "                st.session_state.data_loaded = True\n",
    "                st.session_state.search_engine = search_engine\n",
    "                st.session_state.vocab_df = vocab_df\n",
    "                st.session_state.docs_df = docs_df\n",
    "                st.session_state.inverted_index = inverted_index\n",
    "                st.success(\"Donn√©es charg√©es avec succ√®s!\")\n",
    "            else:\n",
    "                st.error(\" √âchec du chargement des donn√©es\")\n",
    "\n",
    "# Interface principale\n",
    "if not st.session_state.data_loaded:\n",
    "    st.warning(\" Veuillez d'abord charger les donn√©es dans la sidebar\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.info(\"\"\"\n",
    "        ** Fichiers requis:**\n",
    "        - `inverted_index.pkl` - Index invers√©\n",
    "        - `vocab.csv` - Vocabulaire et pond√©rations  \n",
    "        - `docs.csv` - M√©tadonn√©es des documents\n",
    "        \"\"\")\n",
    "    \n",
    "    with col2:\n",
    "        st.info(\"\"\"\n",
    "        ** Instructions:**\n",
    "        1. Cliquez sur *Charger les donn√©es*\n",
    "        2. Attendez le message de confirmation\n",
    "        3. Entrez votre requ√™te m√©dicale\n",
    "        4. Consultez les r√©sultats class√©s\n",
    "        \"\"\")\n",
    "    \n",
    "    # Aper√ßu des fichiers disponibles\n",
    "    st.subheader(\" Fichiers disponibles\")\n",
    "    available_files = []\n",
    "    for file in ['inverted_index.pkl', 'vocab.csv', 'docs.csv']:\n",
    "        if os.path.exists(file):\n",
    "            available_files.append(f\" {file}\")\n",
    "        else:\n",
    "            available_files.append(f\" {file}\")\n",
    "    \n",
    "    st.write(\"\\n\".join(available_files))\n",
    "\n",
    "else:\n",
    "    # Affichage des statistiques\n",
    "    st.sidebar.header(\" Statistiques\")\n",
    "    st.sidebar.metric(\"Documents index√©s\", st.session_state.search_engine.N)\n",
    "    \n",
    "    if st.session_state.vocab_df is not None:\n",
    "        st.sidebar.metric(\"Termes uniques\", len(st.session_state.vocab_df))\n",
    "    \n",
    "    st.sidebar.metric(\"Longueur moyenne\", f\"{st.session_state.search_engine.avg_dl:.0f} tokens\")\n",
    "    \n",
    "    # Section de recherche\n",
    "    st.header(\" Recherche m√©dicale\")\n",
    "    \n",
    "    query = st.text_input(\n",
    "        \"Entrez vos termes de recherche:\",\n",
    "        placeholder=\"ex: ventilation m√©canique sepsis pression art√©rielle...\",\n",
    "        key=\"search_input\"\n",
    "    )\n",
    "    \n",
    "    col1, col2, col3 = st.columns([2, 1, 1])\n",
    "    \n",
    "    with col2:\n",
    "        top_k = st.selectbox(\"R√©sultats par page\", [5, 10, 20, 50], index=1)\n",
    "    \n",
    "    with col3:\n",
    "        search_button = st.button(\" Lancer la recherche\", use_container_width=True)\n",
    "    \n",
    "    # Ex√©cution de la recherche\n",
    "    if search_button or (query and st.session_state.get('last_query') != query):\n",
    "        if query:\n",
    "            st.session_state.last_query = query\n",
    "            \n",
    "            with st.spinner(f\"Recherche de '{query}'...\"):\n",
    "                results, query_terms = st.session_state.search_engine.search(query, top_k=top_k)\n",
    "                \n",
    "                # Affichage des r√©sultats\n",
    "                st.subheader(f\" R√©sultats pour: '{query}'\")\n",
    "                \n",
    "                if query_terms:\n",
    "                    st.write(f\"**Termes recherch√©s:** {', '.join(query_terms)}\")\n",
    "                \n",
    "                if results:\n",
    "                    st.write(f\"**{len(results)} document(s) trouv√©(s)**\")\n",
    "                    \n",
    "                    for i, (doc_id, score, doc_length) in enumerate(results, 1):\n",
    "                        with st.container():\n",
    "                            st.markdown(f\"###  {doc_id} _(score: {score:.4f})_\")\n",
    "                            \n",
    "                            # M√©tadonn√©es\n",
    "                            col_meta1, col_meta2, col_meta3 = st.columns(3)\n",
    "                            \n",
    "                            with col_meta1:\n",
    "                                st.metric(\"Score BM25\", f\"{score:.4f}\")\n",
    "                            \n",
    "                            with col_meta2:\n",
    "                                st.metric(\"Longueur\", f\"{doc_length} tokens\")\n",
    "                            \n",
    "                            with col_meta3:\n",
    "   \n",
    "                                if st.session_state.docs_df is not None:\n",
    "                                    doc_info = st.session_state.docs_df[\n",
    "                                        st.session_state.docs_df['doc_id'] == doc_id\n",
    "                                    ]\n",
    "                                    if not doc_info.empty:\n",
    "                                        st.metric(\"Phrases\", int(doc_info.iloc[0]['num_sentences']))\n",
    "                            \n",
    "                \n",
    "                            st.write(\"**Extrait:**\")\n",
    "                            st.write(f\"*Document m√©dical traitant de {', '.join(query_terms[:3])}. Contenu sp√©cialis√© en r√©animation m√©dicale avec des donn√©es cliniques d√©taill√©es...*\")\n",
    "                            \n",
    "                     \n",
    "                            col_btn1, col_btn2 = st.columns(2)\n",
    "                            \n",
    "                            with col_btn1:\n",
    "                                if st.button(f\" Voir le document complet\", key=f\"view_{doc_id}\"):\n",
    "                                    st.info(f\"Fonctionnalit√© d'affichage complet pour {doc_id} - √Ä impl√©menter\")\n",
    "                            \n",
    "                            with col_btn2:\n",
    "                                if st.button(f\" Analyser les termes\", key=f\"analyze_{doc_id}\"):\n",
    "                             \n",
    "                                    matching_terms = []\n",
    "                                    for term in query_terms:\n",
    "                                        if term in st.session_state.inverted_index and doc_id in st.session_state.inverted_index[term]:\n",
    "                                            tf = st.session_state.inverted_index[term][doc_id]['tf']\n",
    "                                            matching_terms.append(f\"{term} (tf={tf})\")\n",
    "                                    \n",
    "                                    if matching_terms:\n",
    "                                        st.success(f\"**Termes correspondants:** {', '.join(matching_terms)}\")\n",
    "                            \n",
    "                            st.markdown(\"---\")\n",
    "                else:\n",
    "                    st.warning(\"Aucun document trouv√©.\")\n",
    "        \n",
    "        else:\n",
    "            st.info(\"Veuillez entrer une requ√™te de recherche.\")\n",
    "\n",
    "\n",
    "st.sidebar.markdown(\"*Moteur de recherche sp√©cialis√©*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39996ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pr√©paration des livrables finaux...\n",
      " PROJET TERMIN√â !\n",
      "\n",
      " Livrables g√©n√©r√©s:\n",
      " pipeline_complet.py - Script principal\n",
      " medical_search_engine.py - Interface Streamlit\n",
      " index/vocab.csv - Vocabulaire et pond√©rations\n",
      "index/docs.csv - M√©tadonn√©es des documents\n",
      " inverted_index.pkl - Index invers√© complet\n",
      " rapport_synthetique.md - Rapport du projet\n"
     ]
    }
   ],
   "source": [
    "print(\" Pr√©paration des livrables finaux...\")\n",
    "\n",
    "# 1. Script Python complet\n",
    "with open('pipeline_complet.py', 'w', encoding='utf-8') as f:\n",
    "    f.write('''\n",
    "# Pipeline complet d'indexation et recherche m√©dicale\n",
    "# Projet TAL - ESEN\n",
    "# \n",
    "# Ce script contient l'ensemble du pipeline de traitement\n",
    "''')\n",
    "\n",
    "# 2. Dossier index/ avec tous les fichiers\n",
    "import os\n",
    "os.makedirs('index', exist_ok=True)\n",
    "\n",
    "# Sauvegarde des fichiers dans le dossier index\n",
    "df_vocab.to_csv('index/vocab.csv', index=False)\n",
    "\n",
    "# M√©tadonn√©es des documents\n",
    "docs_metadata = []\n",
    "for doc_id, doc_data in corpus_lemmatized.items():\n",
    "    docs_metadata.append({\n",
    "        'doc_id': doc_id,\n",
    "        'num_tokens': len(doc_data['lemmas']),\n",
    "        'num_sentences': len(doc_data['sentences'])\n",
    "    })\n",
    "\n",
    "df_docs = pd.DataFrame(docs_metadata)\n",
    "df_docs.to_csv('index/docs.csv', index=False)\n",
    "\n",
    "# 3. Rapport synth√©tique\n",
    "rapport = \"\"\"\n",
    "# RAPPORT SYNTH√âTIQUE - Moteur de Recherche M√©dicale\n",
    "\n",
    "## √âtapes r√©alis√©es\n",
    "- Chargement et pr√©traitement du corpus m√©dical\n",
    "- Nettoyage linguistique complet (normalisation, tokenisation)\n",
    "- Lemmatisation avec spaCy\n",
    "- Construction du vocabulaire et index invers√©\n",
    "- Impl√©mentation de l'algorithme BM25\n",
    "- Interface Streamlit fonctionnelle\n",
    "\n",
    "## Choix techniques\n",
    "- Utilisation de spaCy pour la lemmatisation (meilleure pr√©cision\n",
    "- BM25 pour le scoring (meilleur que TF-IDF pour la recherche)\n",
    "- Stockage en pickle pour l'index (performance)\n",
    "\n",
    "## Difficult√©s rencontr√©es\n",
    "- Gestion des termes m√©dicaux sp√©cifiques\n",
    "- Optimisation des performances pour les gros corpus\n",
    "- Adaptation des stopwords au domaine m√©dical\n",
    "\n",
    "## Pistes d'am√©lioration\n",
    "- Interface avanc√©e avec filtres\n",
    "- Support des requ√™tes bool√©ennes\n",
    "- Visualisation des r√©sultats\n",
    "\"\"\"\n",
    "\n",
    "with open('rapport_synthetique.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(rapport)\n",
    "\n",
    "print(\" PROJET TERMIN√â !\")\n",
    "print(\"\\n Livrables g√©n√©r√©s:\")\n",
    "print(\" pipeline_complet.py - Script principal\")\n",
    "print(\" medical_search_engine.py - Interface Streamlit\")\n",
    "print(\" index/vocab.csv - Vocabulaire et pond√©rations\")\n",
    "print(\"index/docs.csv - M√©tadonn√©es des documents\")\n",
    "print(\" inverted_index.pkl - Index invers√© complet\")\n",
    "print(\" rapport_synthetique.md - Rapport du projet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
